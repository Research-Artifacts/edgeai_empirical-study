## ğŸ” Step 1 â€” GitHub Mining Script ([api_search.py](./api_search.py))


This script performs a structured mining process over GitHubâ€™s REST API to retrieve repositories that match a predefined 
set of Edge AI-related terms. It extracts key metadata â€”such as commit history, collaborators, stars, programming 
language, and activity over the last yearâ€” and stores the results in timestamped CSV files to enable reproducible 
analysis.

### ğŸ“ Output

For each search term, the script generates a CSV file under:

```
dataset/raw_data/
```

Each file follows the naming pattern:

```
RAW_<term>_repos_<timestamp>.csv
```

These CSVs form the foundational dataset used in the subsequent filtering, cleaning, and thematic analysis phases.

### âš™ï¸ How to Run

1. Ensure that all [project dependencies](../INSTALL.md) are installed using Poetry and Activate the virtual environment:

### ğŸ”’ Authentication

Make sure that your GitHub Personal Access Token (PAT) is set in the `.env` file under the `GITHUB_API_TOKEN` key.
This significantly reduces the chance of rate-limit errors during large-scale mining.

2. Execute the mining script:

   ```bash
   poetry run python api_search.py
   ```

3. The script will automatically:

   * Query the GitHub REST API for all predefined Edge AI-related search terms
   * Count total commits 2024 (can be changed in the script)
   * Count contributors
   * Save all enriched repository records into structured CSV files

---



## ğŸ§© Step 2 â€” Dataset Processing & Cleaning ([data_treatment.py](./data_treatment.py))

After collecting raw repository data using the **GitHub Search API script** (`api_search.py`), the next step is to 
**process and refine the dataset** before performing analysis or visualization.  


### âš™ï¸ Overview

This script performs the following main operations:

|              Option               | Operation                        | Description                                                                                                                       |
|:---------------------------------:|:---------------------------------|:----------------------------------------------------------------------------------------------------------------------------------|
|        **[1] Concatenate**        | Merge multiple raw CSV fragments | Combines files generated by `api_search.py` (stored under `dataset/raw_data`) into a single dataset.                              |
|     **[2] Remove Duplicates**     | Deduplicate repositories         | Removes duplicate entries based on the columns `name`, `full_name`, and `URL`.                                                    |
|  **[3] Filter by Language (EN)**  | Keep only English descriptions   | Detects the language of each repository description and keeps only those written in English.                                      |
| **[4] Filter by Exclusion Terms** | Clean irrelevant repositories    | Excludes repositories containing keywords like `toy`, `tutorial`, `book`, simulator` in their name, description, or search terms. |



1. Execute the treatment script:

```bash
  poetry run python data_treatment.py
```
   

```
    === Main Menu ===
    Select the processing type:
    [1] - Concatenate
    [2] - Remove Duplicates
    [3] - Filter by Language Descriptions
    [4] - Filter by Exclusion Terms
    =================
    Enter your choice:
```

Each operation automatically generates timestamped CSV outputs under: (`dataset/processed_data/`)


```text
    dataset/
    â”œâ”€â”€ raw_data/
    â”‚   â”œâ”€â”€ RAW_*_repos_(timestamp).csv
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ processed_data/
    â”‚   â”œâ”€â”€ [CONCATENATED]-raw_data-(timestamp).csv
    â”‚   â”œâ”€â”€ [NO-DUPLICATED]_repo-files_(timestamp).csv
    â”‚   â”œâ”€â”€ [ENGLISH-DESC]_repo-files_(timestamp).csv
    â”‚   â””â”€â”€ [EXCLUSION-TERM]_edgeai_(timestamp).csv
    â””â”€â”€ _raw_data-[experiment_used]/ (the data used in the experiment)
        â”œâ”€â”€ RAW_*_repos_(2025-01-19).csv
        â””â”€â”€ ...
```