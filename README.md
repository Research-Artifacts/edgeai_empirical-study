# Getting Started

Follow this step-by-step guide to set up your environment, install dependencies with **Poetry**, and run the 
**first script of the study (GitHub API data collection â€” `api_search`)**.

---
## ğŸ§© Step 1 â€” Prepare you Environment



#### 1.1) Prerequisites

- **Git**
- **Python 3.13** (recommended)
- **Poetry 1.8+**

> ğŸ’¡ Tip: if Poetry is not installed yet, use:
> ```bash
>  pipx install poetry
> ```

---

#### 1.2) Clone the repository

```bash
  git clone https://github.com/<org>/<edgeai.empirical-study.replication-package>.git
  cd edgeai.empirical-study.replication-package
```

#### 1.3) Set up environment variables
Create your .env file (used by the data collection scripts):
```shell
  cp .env.example .env
```
Edit .env and set at least the following variables:
```dotenv
GITHUB_TOKEN=ghp_xxx...# token with 'repo' and 'read:org' scopes recommended
GITHUB_API_URL=https://api.github.com
```
âš ï¸ Use a Personal Access Token (classic) with repo and read:org permissions to avoid rate-limit issues during data collection.


#### 1.4) Install dependencies with Poetry
```shell
  poetry lock --no-update
  poetry install
  poetry env activate 
```


## ğŸ§© Step 2 â€” Dataset Processing & Cleaning

After collecting raw repository data using the **GitHub Search API script** (`api_search.py`), the next step is to 
**process and refine the dataset** before performing analysis or visualization.  
This stage is handled by the script:


It provides a **menu-driven interface** that consolidates all data-treatment routines into a single entry point, 
ensuring reproducibility and reducing manual effort.

---

### âš™ï¸ Overview

This script performs the following main operations:

| Option | Operation | Description |
|:------:|:-----------|:-------------|
| **[1] Concatenate** | Merge multiple raw CSV fragments | Combines files generated by `api_search.py` (stored under `dataset/raw_data`) into a single dataset. |
| **[2] Remove Duplicates** | Deduplicate repositories | Removes duplicate entries based on the columns `name`, `full_name`, and `URL`. |
| **[3] Filter by Language (EN)** | Keep only English descriptions | Detects the language of each repository description and keeps only those written in English. |
| **[4] Filter by Exclusion Terms** | Clean irrelevant repositories | Excludes repositories containing keywords like `toy`, `tutorial`, `course`, `demo`, `book`, or `simulator` in their name, description, or search terms. |

Each operation automatically generates timestamped CSV outputs under:


---

### â–¶ï¸ How to Run

1. **Ensure dependencies are installed:**

   ```bash
   pip install -r requirements.txt

python dataset_processing_menu.py

```
=== Main Menu ===
Select the processing type:
[1] - Concatenate
[2] - Remove Duplicates
[3] - Filter by Language Descriptions
[4] - Filter by Exclusion Terms
=================
Enter your choice:
```

```css

dataset/
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ RAW_*_repos_2025-01-19_22:32:38
â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ [CONCATENATED]-raw_data-2025-11-12_14:30:27.csv
â”‚   â”œâ”€â”€ [NO-DUPLICATED]_repo-files_2025-11-12.csv
â”‚   â”œâ”€â”€ [ENGLISH-DESC]_repo-files_2025-11-12.csv
â”‚   â””â”€â”€ [EXCLUSION-TERM]_edgeai_2025-11-12.csv
â””â”€â”€ data_analysis/
    â””â”€â”€ filtered_by_exclusion_criteria/
