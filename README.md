# Introduction
This repository contains the supplementary material and replication package for our empirical study on architectural 
practices and guidelines in Edge AIâ€“based systems. It provides the complete research artifacts, including datasets, 
scripts, and analytical workflows used to collect, process, and analyze open-source EdgeAI repositories. The goal of 
this material is to ensure transparency, reproducibility, and reusability of our findings. Researchers can use the 
included tools to replicate the studyâ€™s pipelineâ€”from dataset acquisition to data treatment and thematic analysisâ€”or 
adapt them for related investigations in software architecture, AI engineering, and edge computing. Together, these 
resources support the broader research community in advancing the understanding and engineering of distributed, 
intelligent systems operating across the cloudâ€“edge continuum.

# Getting Started

Follow this step-by-step guide to set up your environment, install dependencies with **Poetry**, and run the 
** script of the study (GitHub API data collection)**.

â¡ï¸ See the full installation guide in **[INSTALL.md](./INSTALL.md)**.



## ğŸ” Step 1 â€” GitHub Mining Script

This replication package includes an automated script responsible for collecting GitHub repositories related to Edge AI and adjacent topics. This script represents the **second step of the data-collection pipeline**, executed **after** running the API-based search script (`api_search`).

### ğŸ“Œ Purpose

This script performs a structured mining process over GitHubâ€™s REST API to retrieve repositories that match a predefined set of Edge-AI-related terms. It extracts key metadataâ€”such as commit history, collaborators, stars, programming language, and activity over the last yearâ€”and stores the results in timestamped CSV files to enable reproducible analysis.

### ğŸ“ Output

For each search term, the script generates a CSV file under:

```
dataset/raw_data/
```

Each file follows the naming pattern:

```
RAW_<term>_repos_<timestamp>.csv
```

These CSVs form the foundational dataset used in the subsequent filtering, cleaning, and thematic analysis phases.

### âš™ï¸ How to Run

1. Ensure that all project dependencies are installed using Poetry and Activate the virtual environment:

### ğŸ”’ Authentication

Make sure that your GitHub Personal Access Token (PAT) is set in the `headers` inside the `config.py`.
This significantly reduces the chance of rate-limit errors during large-scale mining.

2. Execute the mining script:

   ```bash
   poetry run python path/to/your_script.py
   ```

4. The script will automatically:

   * Query the GitHub REST API for all predefined EdgeAI-related search terms
   * Avoid duplicate results
   * Count commits (total and 2024-specific)
   * Count contributors
   * Save all enriched repository records into structured CSV files

---



## ğŸ§© Step 2 â€” Dataset Processing & Cleaning

After collecting raw repository data using the **GitHub Search API script** (`api_search.py`), the next step is to 
**process and refine the dataset** before performing analysis or visualization.  
This stage is handled by the script:


It provides a **menu-driven interface** that consolidates all data-treatment routines into a single entry point, 
ensuring reproducibility and reducing manual effort.

---

### âš™ï¸ Overview

This script performs the following main operations:

| Option | Operation | Description |
|:------:|:-----------|:-------------|
| **[1] Concatenate** | Merge multiple raw CSV fragments | Combines files generated by `api_search.py` (stored under `dataset/raw_data`) into a single dataset. |
| **[2] Remove Duplicates** | Deduplicate repositories | Removes duplicate entries based on the columns `name`, `full_name`, and `URL`. |
| **[3] Filter by Language (EN)** | Keep only English descriptions | Detects the language of each repository description and keeps only those written in English. |
| **[4] Filter by Exclusion Terms** | Clean irrelevant repositories | Excludes repositories containing keywords like `toy`, `tutorial`, `course`, `demo`, `book`, or `simulator` in their name, description, or search terms. |

Each operation automatically generates timestamped CSV outputs under:



2. Execute the mining script:

```bash
  poetry run python python dataset_processing_menu.py
```
   

```
=== Main Menu ===
Select the processing type:
[1] - Concatenate
[2] - Remove Duplicates
[3] - Filter by Language Descriptions
[4] - Filter by Exclusion Terms
=================
Enter your choice:
```

```css
dataset/
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ RAW_*_repos_2025-01-19_22:32:38
â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ [CONCATENATED]-raw_data-2025-11-12_14:30:27.csv
â”‚   â”œâ”€â”€ [NO-DUPLICATED]_repo-files_2025-11-12.csv
â”‚   â”œâ”€â”€ [ENGLISH-DESC]_repo-files_2025-11-12.csv
â”‚   â””â”€â”€ [EXCLUSION-TERM]_edgeai_2025-11-12.csv
â””â”€â”€ data_analysis/
    â””â”€â”€ filtered_by_exclusion_criteria/
```