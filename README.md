# Introduction
This repository contains the supplementary material and replication package for our empirical study on architectural 
practices and guidelines in Edge AIâ€“based systems. It provides the complete research artifacts, including datasets, 
scripts, and analytical workflows used to collect, process, and analyze open-source Edge AI repositories. The goal of 
this material is to ensure transparency, reproducibility, and reusability of our findings. Researchers can use the 
included tools to replicate the studyâ€™s pipeline â€”from dataset acquisition to data treatment and thematic analysisâ€” or 
adapt them for related investigations in software architecture, AI engineering, and edge computing. Together, these 
resources support the broader research community in advancing the understanding and engineering of distributed, 
intelligent systems operating across the cloudâ€“edge continuum.

---

# Getting Started

Follow this step-by-step guide to set up your environment, install all required dependencies, and prepare the project 
for execution.

â¡ï¸ For a complete installation walkthrough, check **[INSTALL.md](./INSTALL.md)**.

Once your environment is ready, you can proceed with the data collection workflow.

â¡ï¸ The full guide for running the **data collection and processing scripts** is available in
**[data_collection/README.md](./data_collection/README.md)**.

---


[//]: # ()
[//]: # (## ğŸ” Step 1 â€” GitHub Mining Script)

[//]: # ()
[//]: # (This replication package includes an automated script responsible for collecting GitHub repositories related to Edge AI and adjacent topics. This script represents the **second step of the data-collection pipeline**, executed **after** running the API-based search script &#40;`api_search`&#41;.)

[//]: # ()
[//]: # (### ğŸ“Œ Purpose)

[//]: # ()
[//]: # (This script performs a structured mining process over GitHubâ€™s REST API to retrieve repositories that match a predefined set of Edge-AI-related terms. It extracts key metadataâ€”such as commit history, collaborators, stars, programming language, and activity over the last yearâ€”and stores the results in timestamped CSV files to enable reproducible analysis.)

[//]: # ()
[//]: # (### ğŸ“ Output)

[//]: # ()
[//]: # (For each search term, the script generates a CSV file under:)

[//]: # ()
[//]: # (```)

[//]: # (dataset/raw_data/)

[//]: # (```)

[//]: # ()
[//]: # (Each file follows the naming pattern:)

[//]: # ()
[//]: # (```)

[//]: # (RAW_<term>_repos_<timestamp>.csv)

[//]: # (```)

[//]: # ()
[//]: # (These CSVs form the foundational dataset used in the subsequent filtering, cleaning, and thematic analysis phases.)

[//]: # ()
[//]: # (### âš™ï¸ How to Run)

[//]: # ()
[//]: # (1. Ensure that all project dependencies are installed using Poetry and Activate the virtual environment:)

[//]: # ()
[//]: # (### ğŸ”’ Authentication)

[//]: # ()
[//]: # (Make sure that your GitHub Personal Access Token &#40;PAT&#41; is set in the `headers` inside the `config.py`.)

[//]: # (This significantly reduces the chance of rate-limit errors during large-scale mining.)

[//]: # ()
[//]: # (2. Execute the mining script:)

[//]: # ()
[//]: # (   ```bash)

[//]: # (   poetry run python path/to/your_script.py)

[//]: # (   ```)

[//]: # ()
[//]: # (4. The script will automatically:)

[//]: # ()
[//]: # (   * Query the GitHub REST API for all predefined EdgeAI-related search terms)

[//]: # (   * Avoid duplicate results)

[//]: # (   * Count commits &#40;total and 2024-specific&#41;)

[//]: # (   * Count contributors)

[//]: # (   * Save all enriched repository records into structured CSV files)

[//]: # ()
[//]: # (---)

[//]: # ()
[//]: # ()
[//]: # ()
[//]: # (## ğŸ§© Step 2 â€” Dataset Processing & Cleaning)

[//]: # ()
[//]: # (After collecting raw repository data using the **GitHub Search API script** &#40;`api_search.py`&#41;, the next step is to )

[//]: # (**process and refine the dataset** before performing analysis or visualization.  )

[//]: # (This stage is handled by the script:)

[//]: # ()
[//]: # ()
[//]: # (It provides a **menu-driven interface** that consolidates all data-treatment routines into a single entry point, )

[//]: # (ensuring reproducibility and reducing manual effort.)

[//]: # ()
[//]: # (---)

[//]: # ()
[//]: # (### âš™ï¸ Overview)

[//]: # ()
[//]: # (This script performs the following main operations:)

[//]: # ()
[//]: # (| Option | Operation | Description |)

[//]: # (|:------:|:-----------|:-------------|)

[//]: # (| **[1] Concatenate** | Merge multiple raw CSV fragments | Combines files generated by `api_search.py` &#40;stored under `dataset/raw_data`&#41; into a single dataset. |)

[//]: # (| **[2] Remove Duplicates** | Deduplicate repositories | Removes duplicate entries based on the columns `name`, `full_name`, and `URL`. |)

[//]: # (| **[3] Filter by Language &#40;EN&#41;** | Keep only English descriptions | Detects the language of each repository description and keeps only those written in English. |)

[//]: # (| **[4] Filter by Exclusion Terms** | Clean irrelevant repositories | Excludes repositories containing keywords like `toy`, `tutorial`, `course`, `demo`, `book`, or `simulator` in their name, description, or search terms. |)

[//]: # ()
[//]: # (Each operation automatically generates timestamped CSV outputs under:)

[//]: # ()
[//]: # ()
[//]: # ()
[//]: # (2. Execute the mining script:)

[//]: # ()
[//]: # (```bash)

[//]: # (  poetry run python python dataset_processing_menu.py)

[//]: # (```)

[//]: # (   )
[//]: # ()
[//]: # (```)

[//]: # (=== Main Menu ===)

[//]: # (Select the processing type:)

[//]: # ([1] - Concatenate)

[//]: # ([2] - Remove Duplicates)

[//]: # ([3] - Filter by Language Descriptions)

[//]: # ([4] - Filter by Exclusion Terms)

[//]: # (=================)

[//]: # (Enter your choice:)

[//]: # (```)

[//]: # ()
[//]: # (```css)

[//]: # (dataset/)

[//]: # (â”œâ”€â”€ raw_data/)

[//]: # (â”‚   â”œâ”€â”€ RAW_*_repos_2025-01-19_22:32:38)

[//]: # (â”‚   â””â”€â”€ ...)

[//]: # (â”œâ”€â”€ processed_data/)

[//]: # (â”‚   â”œâ”€â”€ [CONCATENATED]-raw_data-2025-11-12_14:30:27.csv)

[//]: # (â”‚   â”œâ”€â”€ [NO-DUPLICATED]_repo-files_2025-11-12.csv)

[//]: # (â”‚   â”œâ”€â”€ [ENGLISH-DESC]_repo-files_2025-11-12.csv)

[//]: # (â”‚   â””â”€â”€ [EXCLUSION-TERM]_edgeai_2025-11-12.csv)

[//]: # (â””â”€â”€ data_analysis/)

[//]: # (    â””â”€â”€ filtered_by_exclusion_criteria/)

[//]: # (```)